Implementing a much simpler example to compare and contrast ols and GD
---

Mainly based on the methods at the end of lctnotes week 39 for a simple case
of a polynomial.

The base implementation is more or less finished. The GD produces mostly similar
results to the ols as well as the skl version.

Next would either be to visualize the simple case, implement the SGD in the GD class.
Or going straight for the franke implementation. 

#finishing simple example and making them OO sort of.
I've made the simple case work for both my GD and SGD code, and compared the results
with skl and direct OLS, at least wrt. to the produced betas. I should probably go through 
the code here and change the "beta"'s to "theta" to comply more with norms for notation

also found likely great source for something in the NN linked in piazza:
https://blog.zhaytam.com/2018/08/15/implement-neural-network-backpropagation/
