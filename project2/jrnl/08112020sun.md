ironing out 2a)
---

The code for 2a now runs, I got runtime warnings when running over the different values.
Seems like they are connected to (after talking with TA) the learningrate. too big learning
rate means you climb further and further away from the minima. 


so discussions abt. organizing, probably a good idea to present a heatmap of some variables,
e.g. learningrate and batchsize. Then mayybe like 3 such, for different number of epochs.

For exploration of the implementation, edge cases are very usefull. Explore too small learnigrate
so that the change is too small and never really gets anywhere. Explore too large learningrate so 
that you step over the minima and explode the values. 
