ironing out 2a)
---

The code for 2a now runs, I got runtime warnings when running over the different values.
Seems like they are connected to (after talking with TA) the learningrate. too big learning
rate means you climb further and further away from the minima. 


so discussions abt. organizing, probably a good idea to present a heatmap of some variables,
e.g. learningrate and batchsize. Then mayybe like 3 such, for different number of epochs.

For exploration of the implementation, edge cases are very usefull. Explore too small learnigrate
so that the change is too small and never really gets anywhere. Explore too large learningrate so 
that you step over the minima and explode the values. 

Tested with simple example as well. The results suggest a decent implementation so far. removing noise 
produces very close results. 

Now to either hash out the specifics of storing and displaying the checks and results for the tests OR
move on to implementing the NN for the further project
