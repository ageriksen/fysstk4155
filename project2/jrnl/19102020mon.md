Cont Newton-Rapson gradient descent
---

continuation from the points mentioned in the entry for
the 18th, looking at the Newton-Raphson method for 
derivation along week 39 lctnts.

##notes, GD:
Using the Newton-Rapson method for the gradient of the 
cost function
use taylor
$ f(s) = 0 = f(x) + (s-x)f'(x) + (s-x)^2/2 f''(x)... $
for well behaved funcs,
$ f(x) + (s-x)f'(x) ~= 0 $
$ s ~= x - f(x)/f'(x) $

$ x[i+1] = x[i] + f(x[i])/f'(x[i]) $

$ x' = ( x(t+h) - x(t) ) / h = v $
$ x[i+1] = x[i] + v[i]*h $
$ v[i+1] = (x[i+1] - x[i])/h $

Steepest descent, Gradient descent. start w initial guess x0
here, the γ(gamma) is referred to as learning rate. 
$ x[k+1] = x[k]−γ[k]∇F(x[k]),  k≥0 $

