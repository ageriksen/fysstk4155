Implementing confidence intervals for betas
-------------------------------------------
So. the confidence intervals of beta can be expressed
as ~ (t - 2\<beta\>, t + 2\<beta\>)
and the \<beta\> can be found by some noise \sigma as well as
the matrix (X^T X)^-1. In question 29 in piazza, Morten has 
included a follow-up where he states this relation and 
> The diagonal elements are the individual Î² variances. 
> Then using the standard definition of the confidence 
> intervals (gaussian distribution) you find the latter as well. 
But I can't quite seem to grasp what "the latter" refers to. 
While the individual beta variances are easy to get, I am confused as
to how to include the unknown noise. 

Could there be that because the true variance of the beta "vector"(?) 
would be the individual beta's scaled by some noise and that this noise would
be part of why we can't say anything fully. Thus our estimation rests in the 
matrix we do possess and use this as the best guestimate for the interval? 

In question 31, followup 31\_f3 Morten states that the \sigma is an unknown 
parameter, which probably shouldn't confuse me as much as it currently does, 
but here we are. so. 

There is an unkown parameter in the estimation and a known one we can estimate.
we take the square root of their product to input into the confidence interval. 

From the expressions, we know that we approximate the outcomes 
y \approx X\beta + \epsilon(0, \sigma)

And the \sigma^2 in the expression for the beta's is the variance of the noise.

I've posted a question on piazza regarding this. We'll se how it pans out. 


Adding stochastic noise to the data
-----------------------------------
Adding in randomness to the input, mainly based on the code from last time. 
Base idea here, is to draw a uniform set of data, but since we want to 
input the data into a terrain simulation, we want the inputs to be ordered.
thus
```python
nrow = 100
ncol = 200
rand_row        =       np.random.uniform(0, 1, size=nrow)
rand_col        =       np.random.uniform(0, 1, size=ncol)

sortrowindex    =       np.argsort(rand_row)
sortcolindex    =       np.argsort(rand_col)

rowsort         =       rand_row[sortrowindex]
colsort         =       rand_col[sortcolindex]
```
And then once we input this into the franke function, we can add noise to 
this as well. previous code for this:
```python
noiseSTR        =       0.1
noise           =       np.random.randn(nrow, ncol)

zmat_true       =       Frankefunction(rowmat, colmat)
zmat            =       zmat_true + noiseSTR*noise
```


Confidence intervals beta
-------------------------
My question @65, Morten indeed agreed that the question was good, that there was
no real way to factor away the noise. If it is unknown, then you might assume a
normal distribution, or maybe other type of noise and some variance. Currently
I think I'll assume that the noise has a variance of 1 (which, I am currently 
creating it, so...) 


Number of data points
---------------------
I noticed that for square matrices, making 1e4 (10 000) data points, then the plot of the
initial surface works, but the program exits with an exit code I forgot to note down, but 
something like 134. 
I imagine it's to do with the size of matrices exceeding ram.

Adding confidence intervals
---------------------------
```python
#variance extracted with the assumption of a variance of 1
var_beta = np.diag(np.linalg.inv(x_train_scaled.t@x_train_scaled))
#confidence intervals [mu - z\sigma/sqrt(n), mu + z\sigma/sqrt(n)], for c=95% -> z=1.96
#according to teachers in piazza, drop the sqrt(n) cause of the \sigma^2 in the expression
#for var(beta). 
z_ = 1.96 # from wikipedia for confidence of 95%
```
that's it for today. 
